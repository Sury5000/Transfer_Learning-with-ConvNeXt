{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tehh-661ahBe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from functools import partial\n",
        "\n",
        "class ResidualUnit(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, stride = 1):\n",
        "    super().__init__()\n",
        "\n",
        "    DefaultConv2d = partial(\n",
        "        nn.Conv2d, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "    self.main_layers = nn.Sequential(\n",
        "        DefaultConv2d(in_channels, out_channels, stride = stride),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(),\n",
        "        DefaultConv2d(out_channels, out_channels),\n",
        "        nn.BatchNorm2d(out_channels)\n",
        "    )\n",
        "\n",
        "    if stride > 1:\n",
        "      self.skip_connection = nn.Sequential(\n",
        "          DefaultConv2d(in_channels, out_channels, kernel_size = 1,\n",
        "                        stride = stride, padding = 0),\n",
        "          nn.BatchNorm2d(out_channels)\n",
        "      )\n",
        "    else:\n",
        "      self.skip_connection = nn.Identity()\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    return F.relu(self.main_layers(inputs) + self.skip_connection(inputs))\n",
        "\n",
        "\n",
        "  # We created Residual Unit which calculates the difference between each residual layers to learn the diff"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Resnet34(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    layers = [\n",
        "        nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 7,\n",
        "                  stride = 2, padding = 3, bias = False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
        "    ]\n",
        "\n",
        "    prev_filter = 64\n",
        "\n",
        "    for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
        "      stride = 1 if filters == prev_filter else 2\n",
        "      layers.append(ResidualUnit(prev_filter, filters, stride))\n",
        "      prev_filter = filters\n",
        "\n",
        "    layers += [\n",
        "        nn.AdaptiveAvgPool2d(output_size = 1),\n",
        "        nn.Flatten(),\n",
        "        nn.LazyLinear(10)\n",
        "    ]\n",
        "\n",
        "    self.resnet = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    return self.resnet(inputs)"
      ],
      "metadata": {
        "id": "cQ9A09g8YjPH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "weights = torchvision.models.ConvNeXt_Base_Weights.IMAGENET1K_V1\n",
        "model = torchvision.models.convnext_base(weights=weights).to(device)"
      ],
      "metadata": {
        "id": "dkhoIE8DfwKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d892322f-c367-472c-cdf0-9846dab79752"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/convnext_base-6075fbad.pth\" to /root/.cache/torch/hub/checkpoints/convnext_base-6075fbad.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 338M/338M [00:01<00:00, 182MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Defaultflowers102 = partial(torchvision.datasets.Flowers102, root = 'datasets',\n",
        "                            transform = weights.transforms(), download = True)\n",
        "\n",
        "train_set = Defaultflowers102(split = 'train')\n",
        "valid_set = Defaultflowers102(split = \"val\")\n",
        "test_set = Defaultflowers102(split = 'val')\n",
        "\n",
        "# We train flowers102 dataset using transfer Learning, this dataset has only 10 images per class which is not enough to learn about a class\n",
        "# We use ConvNext model lower layers and from there we try to learn this dataset"
      ],
      "metadata": {
        "id": "65BKn2FiETr9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ba28410-a2d4-40ae-c8a3-8816f04de002"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 345M/345M [00:17<00:00, 19.4MB/s]\n",
            "100%|██████████| 502/502 [00:00<00:00, 2.14MB/s]\n",
            "100%|██████████| 15.0k/15.0k [00:00<00:00, 22.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size = 32, shuffle = True)\n",
        "valid_loader = DataLoader(valid_set, batch_size = 32)\n",
        "test_loader = DataLoader(test_set, batch_size = 32)"
      ],
      "metadata": {
        "id": "VVOokZExIAPD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[name for name, child in model.named_children()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpHSDaqHJImn",
        "outputId": "2edecdf2-6ae0-496f-d558-3b3772b83ceb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['features', 'avgpool', 'classifier']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.features\n",
        "# the ConvNext Architecture for the lower layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YtNaX6mJkEG",
        "outputId": "962eec0a-de6c-4c1d-d773-1cc21985bf65"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2dNormActivation(\n",
              "    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
              "    (1): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (1): Sequential(\n",
              "    (0): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
              "    )\n",
              "    (1): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.014285714285714285, mode=row)\n",
              "    )\n",
              "    (2): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.02857142857142857, mode=row)\n",
              "    )\n",
              "  )\n",
              "  (2): Sequential(\n",
              "    (0): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
              "    (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              "  (3): Sequential(\n",
              "    (0): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=256, out_features=1024, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=1024, out_features=256, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.04285714285714286, mode=row)\n",
              "    )\n",
              "    (1): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=256, out_features=1024, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=1024, out_features=256, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.05714285714285714, mode=row)\n",
              "    )\n",
              "    (2): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=256, out_features=1024, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=1024, out_features=256, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.07142857142857142, mode=row)\n",
              "    )\n",
              "  )\n",
              "  (4): Sequential(\n",
              "    (0): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)\n",
              "    (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              "  (5): Sequential(\n",
              "    (0): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.08571428571428572, mode=row)\n",
              "    )\n",
              "    (1): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
              "    )\n",
              "    (2): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.11428571428571428, mode=row)\n",
              "    )\n",
              "    (3): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.12857142857142856, mode=row)\n",
              "    )\n",
              "    (4): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.14285714285714285, mode=row)\n",
              "    )\n",
              "    (5): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.15714285714285714, mode=row)\n",
              "    )\n",
              "    (6): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.17142857142857143, mode=row)\n",
              "    )\n",
              "    (7): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.18571428571428572, mode=row)\n",
              "    )\n",
              "    (8): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n",
              "    )\n",
              "    (9): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.21428571428571427, mode=row)\n",
              "    )\n",
              "    (10): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.22857142857142856, mode=row)\n",
              "    )\n",
              "    (11): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.24285714285714285, mode=row)\n",
              "    )\n",
              "    (12): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.2571428571428571, mode=row)\n",
              "    )\n",
              "    (13): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.2714285714285714, mode=row)\n",
              "    )\n",
              "    (14): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.2857142857142857, mode=row)\n",
              "    )\n",
              "    (15): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.3, mode=row)\n",
              "    )\n",
              "    (16): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.3142857142857143, mode=row)\n",
              "    )\n",
              "    (17): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.32857142857142857, mode=row)\n",
              "    )\n",
              "    (18): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.34285714285714286, mode=row)\n",
              "    )\n",
              "    (19): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.35714285714285715, mode=row)\n",
              "    )\n",
              "    (20): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.37142857142857144, mode=row)\n",
              "    )\n",
              "    (21): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.38571428571428573, mode=row)\n",
              "    )\n",
              "    (22): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.4, mode=row)\n",
              "    )\n",
              "    (23): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.4142857142857143, mode=row)\n",
              "    )\n",
              "    (24): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.42857142857142855, mode=row)\n",
              "    )\n",
              "    (25): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.44285714285714284, mode=row)\n",
              "    )\n",
              "    (26): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.45714285714285713, mode=row)\n",
              "    )\n",
              "  )\n",
              "  (6): Sequential(\n",
              "    (0): LayerNorm2d((512,), eps=1e-06, elementwise_affine=True)\n",
              "    (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              "  (7): Sequential(\n",
              "    (0): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.4714285714285714, mode=row)\n",
              "    )\n",
              "    (1): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.4857142857142857, mode=row)\n",
              "    )\n",
              "    (2): CNBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
              "        (1): Permute()\n",
              "        (2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (3): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (4): GELU(approximate='none')\n",
              "        (5): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (6): Permute()\n",
              "      )\n",
              "      (stochastic_depth): StochasticDepth(p=0.5, mode=row)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.classifier\n",
        "\n",
        "# Here this model was trained on Imagenet which is 1000 class dataset we need to change this part to work with our dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlYk8eh1JsAG",
        "outputId": "3977d3cb-a7b9-4410-fdca-7f5cec531835"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): LayerNorm2d((1024,), eps=1e-06, elementwise_affine=True)\n",
              "  (1): Flatten(start_dim=1, end_dim=-1)\n",
              "  (2): Linear(in_features=1024, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = 102\n",
        "model.classifier[2] = nn.Linear(1024, n_classes).to(device)"
      ],
      "metadata": {
        "id": "HTDcO5msJ2oA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.classifier\n",
        "# Now we changed the output head to match our class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ndit8bnhKgDW",
        "outputId": "3096a8da-d7aa-45ed-9672-6505ec6e5569"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): LayerNorm2d((1024,), eps=1e-06, elementwise_affine=True)\n",
              "  (1): Flatten(start_dim=1, end_dim=-1)\n",
              "  (2): Linear(in_features=1024, out_features=102, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "for param in model.classifier.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "# We freeze the model parameters except for the classifer we cahnged to learn the image for this dataset\n",
        "# enabling earlier layers to learn will cause overfitting because of our small dataset"
      ],
      "metadata": {
        "id": "GIgqNwkqKiGA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms.v2 as T\n",
        "\n",
        "transforms = T.Compose([\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomRotation(degrees = 30),\n",
        "    T.RandomResizedCrop(size = (224,224), scale = (0.8, 1.0)),\n",
        "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue = 0.1),\n",
        "    T.ToImage(),\n",
        "    T.ToDtype(torch.float32, scale = True),\n",
        "    T.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "xBZRS8VAPYKW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=1e-3)\n",
        "\n",
        "# Reduce LR if validation loss stops improving\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    patience=2\n",
        ")"
      ],
      "metadata": {
        "id": "ryVZvC-xRgKj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n"
      ],
      "metadata": {
        "id": "h3NnVYbDSsUN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return running_loss / total, correct / total"
      ],
      "metadata": {
        "id": "gBWO4xkhJ_zk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return running_loss / total, correct / total"
      ],
      "metadata": {
        "id": "nITDYWnPKEXo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "early_stopping = EarlyStopping(patience=5)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        model, train_loader, optimizer, criterion, device\n",
        "    )\n",
        "\n",
        "    val_loss, val_acc = validate(\n",
        "        model, valid_loader, criterion, device\n",
        "    )\n",
        "\n",
        "    # Step scheduler using validation loss\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "    # Early stopping check\n",
        "    early_stopping(val_loss)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiiMspipKG4y",
        "outputId": "7dbebb2e-d261-46da-c211-0410d115f4f6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30]\n",
            "Train Loss: 4.3064 | Train Acc: 0.1422\n",
            "Val   Loss: 3.4932 | Val   Acc: 0.5647\n",
            "--------------------------------------------------\n",
            "Epoch [2/30]\n",
            "Train Loss: 2.9995 | Train Acc: 0.7157\n",
            "Val   Loss: 2.5210 | Val   Acc: 0.7373\n",
            "--------------------------------------------------\n",
            "Epoch [3/30]\n",
            "Train Loss: 2.0594 | Train Acc: 0.8333\n",
            "Val   Loss: 1.7991 | Val   Acc: 0.8235\n",
            "--------------------------------------------------\n",
            "Epoch [4/30]\n",
            "Train Loss: 1.3858 | Train Acc: 0.9167\n",
            "Val   Loss: 1.3369 | Val   Acc: 0.8608\n",
            "--------------------------------------------------\n",
            "Epoch [5/30]\n",
            "Train Loss: 0.9548 | Train Acc: 0.9461\n",
            "Val   Loss: 1.0694 | Val   Acc: 0.8647\n",
            "--------------------------------------------------\n",
            "Epoch [6/30]\n",
            "Train Loss: 0.6896 | Train Acc: 0.9520\n",
            "Val   Loss: 0.8837 | Val   Acc: 0.8784\n",
            "--------------------------------------------------\n",
            "Epoch [7/30]\n",
            "Train Loss: 0.5072 | Train Acc: 0.9667\n",
            "Val   Loss: 0.7654 | Val   Acc: 0.8833\n",
            "--------------------------------------------------\n",
            "Epoch [8/30]\n",
            "Train Loss: 0.3934 | Train Acc: 0.9833\n",
            "Val   Loss: 0.6867 | Val   Acc: 0.8892\n",
            "--------------------------------------------------\n",
            "Epoch [9/30]\n",
            "Train Loss: 0.3149 | Train Acc: 0.9843\n",
            "Val   Loss: 0.6227 | Val   Acc: 0.8931\n",
            "--------------------------------------------------\n",
            "Epoch [10/30]\n",
            "Train Loss: 0.2475 | Train Acc: 0.9882\n",
            "Val   Loss: 0.5799 | Val   Acc: 0.8941\n",
            "--------------------------------------------------\n",
            "Epoch [11/30]\n",
            "Train Loss: 0.2127 | Train Acc: 0.9931\n",
            "Val   Loss: 0.5409 | Val   Acc: 0.8941\n",
            "--------------------------------------------------\n",
            "Epoch [12/30]\n",
            "Train Loss: 0.1801 | Train Acc: 0.9931\n",
            "Val   Loss: 0.5153 | Val   Acc: 0.8990\n",
            "--------------------------------------------------\n",
            "Epoch [13/30]\n",
            "Train Loss: 0.1512 | Train Acc: 0.9931\n",
            "Val   Loss: 0.4904 | Val   Acc: 0.8980\n",
            "--------------------------------------------------\n",
            "Epoch [14/30]\n",
            "Train Loss: 0.1261 | Train Acc: 0.9971\n",
            "Val   Loss: 0.4636 | Val   Acc: 0.8961\n",
            "--------------------------------------------------\n",
            "Epoch [15/30]\n",
            "Train Loss: 0.1085 | Train Acc: 0.9980\n",
            "Val   Loss: 0.4527 | Val   Acc: 0.8980\n",
            "--------------------------------------------------\n",
            "Epoch [16/30]\n",
            "Train Loss: 0.0988 | Train Acc: 0.9990\n",
            "Val   Loss: 0.4372 | Val   Acc: 0.9020\n",
            "--------------------------------------------------\n",
            "Epoch [17/30]\n",
            "Train Loss: 0.0848 | Train Acc: 1.0000\n",
            "Val   Loss: 0.4283 | Val   Acc: 0.8980\n",
            "--------------------------------------------------\n",
            "Epoch [18/30]\n",
            "Train Loss: 0.0817 | Train Acc: 0.9990\n",
            "Val   Loss: 0.4195 | Val   Acc: 0.9000\n",
            "--------------------------------------------------\n",
            "Epoch [19/30]\n",
            "Train Loss: 0.0734 | Train Acc: 0.9980\n",
            "Val   Loss: 0.4085 | Val   Acc: 0.9000\n",
            "--------------------------------------------------\n",
            "Epoch [20/30]\n",
            "Train Loss: 0.0584 | Train Acc: 1.0000\n",
            "Val   Loss: 0.4000 | Val   Acc: 0.9029\n",
            "--------------------------------------------------\n",
            "Epoch [21/30]\n",
            "Train Loss: 0.0559 | Train Acc: 1.0000\n",
            "Val   Loss: 0.3908 | Val   Acc: 0.9029\n",
            "--------------------------------------------------\n",
            "Epoch [22/30]\n",
            "Train Loss: 0.0547 | Train Acc: 0.9990\n",
            "Val   Loss: 0.3820 | Val   Acc: 0.9078\n",
            "--------------------------------------------------\n",
            "Epoch [23/30]\n",
            "Train Loss: 0.0476 | Train Acc: 1.0000\n",
            "Val   Loss: 0.3761 | Val   Acc: 0.9078\n",
            "--------------------------------------------------\n",
            "Epoch [24/30]\n",
            "Train Loss: 0.0460 | Train Acc: 0.9990\n",
            "Val   Loss: 0.3753 | Val   Acc: 0.9069\n",
            "--------------------------------------------------\n",
            "Epoch [25/30]\n",
            "Train Loss: 0.0364 | Train Acc: 1.0000\n",
            "Val   Loss: 0.3707 | Val   Acc: 0.9069\n",
            "--------------------------------------------------\n",
            "Epoch [26/30]\n",
            "Train Loss: 0.0368 | Train Acc: 1.0000\n",
            "Val   Loss: 0.3670 | Val   Acc: 0.9069\n",
            "--------------------------------------------------\n",
            "Epoch [27/30]\n",
            "Train Loss: 0.0339 | Train Acc: 1.0000\n",
            "Val   Loss: 0.3650 | Val   Acc: 0.9088\n",
            "--------------------------------------------------\n",
            "Epoch [28/30]\n",
            "Train Loss: 0.0329 | Train Acc: 1.0000\n",
            "Val   Loss: 0.3610 | Val   Acc: 0.9059\n",
            "--------------------------------------------------\n",
            "Epoch [29/30]\n",
            "Train Loss: 0.0297 | Train Acc: 1.0000\n",
            "Val   Loss: 0.3514 | Val   Acc: 0.9059\n",
            "--------------------------------------------------\n",
            "Epoch [30/30]\n",
            "Train Loss: 0.0295 | Train Acc: 0.9990\n",
            "Val   Loss: 0.3513 | Val   Acc: 0.9059\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}